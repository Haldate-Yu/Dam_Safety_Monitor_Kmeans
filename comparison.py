import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
import os
import csv
import improved_kmeans
import origin_kmeans
import random_kmeans

# å…¨å±€é…ç½®
matplotlib.rcParams['font.sans-serif'] = ['SimHei']
matplotlib.rcParams['axes.unicode_minus'] = False
OUTPUT_DIR = "cluster_comparison_results"
os.makedirs(OUTPUT_DIR, exist_ok=True)


# 1. è¯»å–ä¸¤ç§æ–¹æ³•çš„ç»“æœï¼ˆéœ€ç¡®ä¿å‰ä¸¤ä¸ªæ–‡ä»¶å·²è¿è¡Œï¼‰
def load_results():
    """è¯»å–åŸå§‹KMeanså’Œæ”¹è¿›KMeansçš„èšç±»ç»“æœã€è€¦åˆç‰¹å¾"""
    # åŸå§‹KMeansç»“æœ
    original_result_path = "original_kmeans_results/åŸå§‹KMeansèšç±»ç»“æœ.xlsx"
    original_coupling_path = "original_kmeans_results/åŸå§‹KMeansè€¦åˆç‰¹å¾.xlsx"
    # éšæœºKMeansç»“æœ
    random_result_path = "improved_kmeans_results/éšæœºKMeansèšç±»ç»“æœ.xlsx"
    random_coupling_path = "improved_kmeans_results/éšæœºKMeansè€¦åˆç‰¹å¾.xlsx"
    # æ”¹è¿›KMeansç»“æœ
    improved_result_path = "improved_kmeans_results/æ”¹è¿›KMeansèšç±»ç»“æœ.xlsx"
    improved_coupling_path = "improved_kmeans_results/æ”¹è¿›KMeansè€¦åˆç‰¹å¾.xlsx"

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    required_files = [original_result_path, original_coupling_path, improved_result_path, improved_coupling_path]
    for file in required_files:
        if not os.path.exists(file):
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨ï¼š{file}ï¼Œè¯·å…ˆè¿è¡ŒåŸå§‹KMeanså’Œæ”¹è¿›KMeansè„šæœ¬")

    # è¯»å–æ•°æ®
    original_result = pd.read_excel(original_result_path, index_col=0)
    original_coupling = pd.read_excel(original_coupling_path)
    random_result = pd.read_excel(random_result_path, index_col=0)
    random_coupling = pd.read_excel(random_coupling_path)
    improved_result = pd.read_excel(improved_result_path, index_col=0)
    improved_coupling = pd.read_excel(improved_coupling_path)

    print("âœ… æˆåŠŸè¯»å–ä¸¤ç§æ–¹æ³•çš„ç»“æœæ•°æ®")
    return random_result, random_coupling, improved_result, improved_coupling
    # return original_result, original_coupling, improved_result, improved_coupling


def load_results_generic(result_a_path, coupling_a_path, result_b_path, coupling_b_path):
    for p in [result_a_path, coupling_a_path, result_b_path, coupling_b_path]:
        if not os.path.exists(p):
            raise FileNotFoundError(p)
    result_a = pd.read_excel(result_a_path, index_col=0)
    coupling_a = pd.read_excel(coupling_a_path)
    result_b = pd.read_excel(result_b_path, index_col=0)
    coupling_b = pd.read_excel(coupling_b_path)
    return result_a, coupling_a, result_b, coupling_b


# 2. è®¡ç®—èšç±»æœ‰æ•ˆæ€§æŒ‡æ ‡ï¼ˆé€šç”¨æŒ‡æ ‡ï¼‰
def load_time_series_data(excel_path="./monitor_data.xlsx"):
    """è¯»å–åŸå§‹ç›‘æµ‹Excelï¼Œè¿”å›{æµ‹ç‚¹åç§°: æ—¶åºSeries}ï¼Œæµ‹ç‚¹åç§°ç»Ÿä¸€ä¸ºå°å†™"""
    xls = pd.ExcelFile(excel_path)
    sheet_data = {}
    for sheet_name in xls.sheet_names:
        try:
            df = pd.read_excel(xls, sheet_name=sheet_name, usecols=[0, 1])
            df.columns = ['TM', 'Z']
            df['TM'] = pd.to_datetime(df['TM'], errors='coerce')
            df = df.dropna(subset=['TM', 'Z'])
            df = df.set_index('TM')
            df = df[~df.index.duplicated(keep='first')]
            sheet_data[sheet_name.lower()] = df['Z']
        except Exception:
            continue
    return sheet_data


def _static_distance(p1, p2, sheet_data):
    """é™æ€è¿‘é‚»è·ç¦»ï¼šä¸¤æµ‹ç‚¹å…±åŒæ—¶é—´ç´¢å¼•çš„æ—¶åºæ¬§æ°è·ç¦»"""
    if p1 not in sheet_data or p2 not in sheet_data:
        return np.nan
    idx = sheet_data[p1].index.intersection(sheet_data[p2].index)
    if len(idx) < 2:
        return np.nan
    x1 = sheet_data[p1].loc[idx].values
    x2 = sheet_data[p2].loc[idx].values
    return float(np.sqrt(np.mean((x1 - x2) ** 2)))


def _compute_static_silhouette_for_result(result_df, sheet_data):
    """åŸºäºé™æ€è¿‘é‚»è·ç¦»çš„è½®å»“ç³»æ•°ï¼ˆè¶Šæ¥è¿‘1è¶Šå¥½ï¼‰"""
    points = result_df.index.tolist()
    labels = result_df['ç°‡æ ‡ç­¾'].to_dict()
    clusters = sorted(result_df['ç°‡æ ‡ç­¾'].unique())
    # é¢„è®¡ç®—è·ç¦»çŸ©é˜µ
    dist = {p: {} for p in points}
    for i, p1 in enumerate(points):
        for j, p2 in enumerate(points):
            if i == j:
                dist[p1][p2] = 0.0
            elif p2 in dist[p1]:
                continue
            else:
                d = _static_distance(p1, p2, sheet_data)
                dist[p1][p2] = d
                dist[p2][p1] = d
    s_list = []
    for p in points:
        same = [q for q in points if labels[q] == labels[p] and q != p]
        a_vals = [dist[p][q] for q in same if not np.isnan(dist[p][q])]
        a = np.mean(a_vals) if len(a_vals) > 0 else 0.0
        b_candidates = []
        for c in clusters:
            if c == labels[p]:
                continue
            other = [q for q in points if labels[q] == c]
            b_vals = [dist[p][q] for q in other if not np.isnan(dist[p][q])]
            if len(b_vals) > 0:
                b_candidates.append(np.mean(b_vals))
        b = min(b_candidates) if len(b_candidates) > 0 else np.nan
        if np.isnan(b) or (a == 0 and b == 0):
            s = 0.0
        else:
            denom = max(a, b)
            s = (b - a) / denom if denom > 0 else 0.0
        s_list.append(s)
    return float(np.mean(s_list)) if len(s_list) > 0 else 0.0


def compute_validity_metrics(original_result, improved_result, sheet_data=None):
    """
    è®¡ç®—3ä¸ªç»å…¸èšç±»æœ‰æ•ˆæ€§æŒ‡æ ‡ï¼š
    1. è½®å»“ç³»æ•°ï¼ˆSilhouetteï¼‰ï¼šè¶Šæ¥è¿‘1è¶Šå¥½ï¼ˆç°‡å†…ç´§å‡‘+ç°‡é—´åˆ†ç¦»ï¼‰
    2. Calinski-Harabaszï¼šè¶Šå¤§è¶Šå¥½ï¼ˆç°‡é—´æ–¹å·®/ç°‡å†…æ–¹å·®ï¼‰
    3. Davies-Bouldinï¼šè¶Šæ¥è¿‘0è¶Šå¥½ï¼ˆç°‡å†…åˆ†æ•£åº¦/ç°‡é—´è·ç¦»ï¼‰
    """
    # æå–ç‰¹å¾å’Œæ ‡ç­¾ï¼ˆæ’é™¤'ç°‡æ ‡ç­¾'åˆ—ï¼‰
    original_feats = original_result.drop('ç°‡æ ‡ç­¾', axis=1).values
    original_labels = original_result['ç°‡æ ‡ç­¾'].values
    improved_feats = improved_result.drop('ç°‡æ ‡ç­¾', axis=1).values
    improved_labels = improved_result['ç°‡æ ‡ç­¾'].values

    # è®¡ç®—åŸå§‹KMeansæŒ‡æ ‡
    original_sil = silhouette_score(original_feats, original_labels) if len(np.unique(original_labels)) > 1 else 0
    original_ch = calinski_harabasz_score(original_feats, original_labels) if len(np.unique(original_labels)) > 1 else 0
    original_db = davies_bouldin_score(original_feats, original_labels) if len(np.unique(original_labels)) > 1 else 0

    # è®¡ç®—æ”¹è¿›KMeansæŒ‡æ ‡
    improved_sil = silhouette_score(improved_feats, improved_labels) if len(np.unique(improved_labels)) > 1 else 0
    improved_ch = calinski_harabasz_score(improved_feats, improved_labels) if len(np.unique(improved_labels)) > 1 else 0
    improved_db = davies_bouldin_score(improved_feats, improved_labels) if len(np.unique(improved_labels)) > 1 else 0

    # æ•´ç†ç»“æœï¼ˆé€šç”¨ä¸‰æŒ‡æ ‡ï¼‰
    validity_df = pd.DataFrame({
        'æŒ‡æ ‡åç§°': ['è½®å»“ç³»æ•°ï¼ˆSilhouetteï¼‰', 'Calinski-Harabasz', 'Davies-Bouldin'],
        'åŸå§‹KMeans': [round(original_sil, 4), round(original_ch, 2), round(original_db, 4)],
        'æ”¹è¿›KMeans': [round(improved_sil, 4), round(improved_ch, 2), round(improved_db, 4)],
        'æŒ‡æ ‡è¯´æ˜': [
            'è¶Šæ¥è¿‘1è¶Šå¥½ï¼ˆç°‡å†…ç´§å‡‘+ç°‡é—´åˆ†ç¦»ï¼‰',
            'è¶Šå¤§è¶Šå¥½ï¼ˆç°‡é—´æ–¹å·®/ç°‡å†…æ–¹å·®æ¯”ï¼‰',
            'è¶Šæ¥è¿‘0è¶Šå¥½ï¼ˆç°‡å†…åˆ†æ•£åº¦/ç°‡é—´è·ç¦»æ¯”ï¼‰'
        ]
    })
    # å¢åŠ ï¼šé™æ€è½®å»“ç³»æ•°ï¼ˆåŸºäºæ—¶åºé™æ€è¿‘é‚»è·ç¦»ï¼‰
    if sheet_data is None:
        try:
            sheet_data = load_time_series_data()
        except Exception:
            sheet_data = None
    if sheet_data is not None and len(sheet_data) > 0:
        original_static_sil = _compute_static_silhouette_for_result(original_result, sheet_data)
        improved_static_sil = _compute_static_silhouette_for_result(improved_result, sheet_data)
        extra_row = pd.DataFrame({
            'æŒ‡æ ‡åç§°': ['é™æ€è½®å»“ç³»æ•°ï¼ˆStatic-Silhouetteï¼‰'],
            'åŸå§‹KMeans': [round(original_static_sil, 4)],
            'æ”¹è¿›KMeans': [round(improved_static_sil, 4)],
            'æŒ‡æ ‡è¯´æ˜': ['è¶Šæ¥è¿‘1è¶Šå¥½ï¼ˆåŸºäºé™æ€è¿‘é‚»è·ç¦»çš„ç°‡å†…/ç°‡é—´å¯¹æ¯”ï¼‰']
        })
        validity_df = pd.concat([validity_df, extra_row], ignore_index=True)
    return validity_df


def compute_validity_metrics_named(result_a, result_b, sheet_data=None, name_a='æ–¹æ³•A', name_b='æ–¹æ³•B'):
    feats_a = result_a.drop('ç°‡æ ‡ç­¾', axis=1).values
    labels_a = result_a['ç°‡æ ‡ç­¾'].values
    feats_b = result_b.drop('ç°‡æ ‡ç­¾', axis=1).values
    labels_b = result_b['ç°‡æ ‡ç­¾'].values
    sil_a = silhouette_score(feats_a, labels_a) if len(np.unique(labels_a)) > 1 else 0
    ch_a = calinski_harabasz_score(feats_a, labels_a) if len(np.unique(labels_a)) > 1 else 0
    db_a = davies_bouldin_score(feats_a, labels_a) if len(np.unique(labels_a)) > 1 else 0
    sil_b = silhouette_score(feats_b, labels_b) if len(np.unique(labels_b)) > 1 else 0
    ch_b = calinski_harabasz_score(feats_b, labels_b) if len(np.unique(labels_b)) > 1 else 0
    db_b = davies_bouldin_score(feats_b, labels_b) if len(np.unique(labels_b)) > 1 else 0
    df = pd.DataFrame({
        'æŒ‡æ ‡åç§°': ['è½®å»“ç³»æ•°ï¼ˆSilhouetteï¼‰', 'Calinski-Harabasz', 'Davies-Bouldin'],
        name_a: [round(sil_a, 4), round(ch_a, 2), round(db_a, 4)],
        name_b: [round(sil_b, 4), round(ch_b, 2), round(db_b, 4)],
        'æŒ‡æ ‡è¯´æ˜': [
            'è¶Šæ¥è¿‘1è¶Šå¥½ï¼ˆç°‡å†…ç´§å‡‘+ç°‡é—´åˆ†ç¦»ï¼‰',
            'è¶Šå¤§è¶Šå¥½ï¼ˆç°‡é—´æ–¹å·®/ç°‡å†…æ–¹å·®æ¯”ï¼‰',
            'è¶Šæ¥è¿‘0è¶Šå¥½ï¼ˆç°‡å†…åˆ†æ•£åº¦/ç°‡é—´è·ç¦»æ¯”ï¼‰'
        ]
    })
    if sheet_data is None:
        try:
            sheet_data = load_time_series_data()
        except Exception:
            sheet_data = None
    if sheet_data is not None and len(sheet_data) > 0:
        s_a = _compute_static_silhouette_for_result(result_a, sheet_data)
        s_b = _compute_static_silhouette_for_result(result_b, sheet_data)
        extra = pd.DataFrame({
            'æŒ‡æ ‡åç§°': ['é™æ€è½®å»“ç³»æ•°ï¼ˆStatic-Silhouetteï¼‰'],
            name_a: [round(s_a, 4)],
            name_b: [round(s_b, 4)],
            'æŒ‡æ ‡è¯´æ˜': ['è¶Šæ¥è¿‘1è¶Šå¥½ï¼ˆåŸºäºé™æ€è¿‘é‚»è·ç¦»çš„ç°‡å†…/ç°‡é—´å¯¹æ¯”ï¼‰']
        })
        df = pd.concat([df, extra], ignore_index=True)
    return df


# 3. è®¡ç®—ä¸šåŠ¡é€‚é…æŒ‡æ ‡ï¼ˆå¤§åæµ‹ç‚¹åœºæ™¯ä¸“å±ï¼‰
def compute_business_metrics(original_coupling, improved_coupling):
    """
    åŸºäºæŠ€æœ¯äº¤åº•ä¹¦çš„ç©ºé—´è€¦åˆæ€§è¦æ±‚ï¼Œè®¡ç®—2ä¸ªä¸šåŠ¡æŒ‡æ ‡ï¼š
    1. ç°‡å†…å¹³å‡æ—¶åºç›¸å…³ç³»æ•°ï¼šè¶Šå¤§è¶Šå¥½ï¼ˆä½“ç°æµ‹ç‚¹æ—¶åºåŒæ­¥æ€§ï¼‰
    2. ç°‡å†…å¹³å‡é™æ€è¿‘é‚»è·ç¦»ï¼šè¶Šå°è¶Šå¥½ï¼ˆä½“ç°æµ‹ç‚¹æ•°å€¼ä¸€è‡´æ€§ï¼‰
    """
    # è¿‡æ»¤æ— æ•ˆå€¼ï¼ˆæ’é™¤NaNï¼‰
    original_coupling_valid = original_coupling.dropna(subset=['é™æ€è¿‘é‚»è·ç¦»', 'æ—¶åºç›¸å…³ç³»æ•°'])
    improved_coupling_valid = improved_coupling.dropna(subset=['é™æ€è¿‘é‚»è·ç¦»', 'æ—¶åºç›¸å…³ç³»æ•°'])

    if len(original_coupling_valid) == 0 or len(improved_coupling_valid) == 0:
        raise ValueError("è€¦åˆç‰¹å¾æ•°æ®ä¸­æ— æœ‰æ•ˆå€¼ï¼Œæ— æ³•è®¡ç®—ä¸šåŠ¡æŒ‡æ ‡")

    # åŸå§‹KMeansä¸šåŠ¡æŒ‡æ ‡
    original_avg_corr = original_coupling_valid['æ—¶åºç›¸å…³ç³»æ•°'].mean()
    original_avg_dist = original_coupling_valid['é™æ€è¿‘é‚»è·ç¦»'].mean()

    # æ”¹è¿›KMeansä¸šåŠ¡æŒ‡æ ‡
    improved_avg_corr = improved_coupling_valid['æ—¶åºç›¸å…³ç³»æ•°'].mean()
    improved_avg_dist = improved_coupling_valid['é™æ€è¿‘é‚»è·ç¦»'].mean()

    # æ•´ç†ç»“æœ
    business_df = pd.DataFrame({
        'æŒ‡æ ‡åç§°': ['ç°‡å†…å¹³å‡æ—¶åºç›¸å…³ç³»æ•°', 'ç°‡å†…å¹³å‡é™æ€è¿‘é‚»è·ç¦»'],
        'åŸå§‹KMeans': [round(original_avg_corr, 4), round(original_avg_dist, 4)],
        'æ”¹è¿›KMeans': [round(improved_avg_corr, 4), round(improved_avg_dist, 4)],
        'æŒ‡æ ‡è¯´æ˜': [
            'è¶Šå¤§è¶Šå¥½ï¼ˆä½“ç°æµ‹ç‚¹æ—¶åºåŒæ­¥æ€§ï¼Œç¬¦åˆå¤§åç©ºé—´è€¦åˆæ€§ï¼‰',
            'è¶Šå°è¶Šå¥½ï¼ˆä½“ç°æµ‹ç‚¹æ•°å€¼ä¸€è‡´æ€§ï¼Œç¬¦åˆå¤§åç©ºé—´è€¦åˆæ€§ï¼‰'
        ]
    })
    return business_df


def compute_business_metrics_named(coupling_a, coupling_b, name_a='æ–¹æ³•A', name_b='æ–¹æ³•B'):
    a_valid = coupling_a.dropna(subset=['é™æ€è¿‘é‚»è·ç¦»', 'æ—¶åºç›¸å…³ç³»æ•°'])
    b_valid = coupling_b.dropna(subset=['é™æ€è¿‘é‚»è·ç¦»', 'æ—¶åºç›¸å…³ç³»æ•°'])
    if len(a_valid) == 0 or len(b_valid) == 0:
        raise ValueError("è€¦åˆç‰¹å¾æ•°æ®ä¸­æ— æœ‰æ•ˆå€¼")
    a_corr = a_valid['æ—¶åºç›¸å…³ç³»æ•°'].mean()
    a_dist = a_valid['é™æ€è¿‘é‚»è·ç¦»'].mean()
    b_corr = b_valid['æ—¶åºç›¸å…³ç³»æ•°'].mean()
    b_dist = b_valid['é™æ€è¿‘é‚»è·ç¦»'].mean()
    return pd.DataFrame({
        'æŒ‡æ ‡åç§°': ['ç°‡å†…å¹³å‡æ—¶åºç›¸å…³ç³»æ•°', 'ç°‡å†…å¹³å‡é™æ€è¿‘é‚»è·ç¦»'],
        name_a: [round(a_corr, 4), round(a_dist, 4)],
        name_b: [round(b_corr, 4), round(b_dist, 4)],
        'æŒ‡æ ‡è¯´æ˜': [
            'è¶Šå¤§è¶Šå¥½ï¼ˆä½“ç°æµ‹ç‚¹æ—¶åºåŒæ­¥æ€§ï¼Œç¬¦åˆå¤§åç©ºé—´è€¦åˆæ€§ï¼‰',
            'è¶Šå°è¶Šå¥½ï¼ˆä½“ç°æµ‹ç‚¹æ•°å€¼ä¸€è‡´æ€§ï¼Œç¬¦åˆå¤§åç©ºé—´è€¦åˆæ€§ï¼‰'
        ]
    })


# 4. ç”Ÿæˆå¯¹æ¯”è¡¨æ ¼ä¸å¯è§†åŒ–
def generate_comparison(validity_df, business_df):
    """
    1. ä¿å­˜å¯¹æ¯”è¡¨æ ¼åˆ°Excel
    2. ç”Ÿæˆå¯è§†åŒ–å¯¹æ¯”å›¾ï¼ˆæŸ±çŠ¶å›¾+é›·è¾¾å›¾ï¼‰
    """
    # 1. ä¿å­˜å¯¹æ¯”è¡¨æ ¼
    with pd.ExcelWriter(os.path.join(OUTPUT_DIR, "èšç±»æ–¹æ³•å¯¹æ¯”æŒ‡æ ‡è¡¨.xlsx"), engine='openpyxl') as writer:
        validity_df.to_excel(writer, sheet_name='èšç±»æœ‰æ•ˆæ€§æŒ‡æ ‡', index=False)
        business_df.to_excel(writer, sheet_name='ä¸šåŠ¡é€‚é…æŒ‡æ ‡', index=False)
    print(f"ğŸ“Š å¯¹æ¯”è¡¨æ ¼å·²ä¿å­˜è‡³ï¼š{os.path.join(OUTPUT_DIR, 'èšç±»æ–¹æ³•å¯¹æ¯”æŒ‡æ ‡è¡¨.xlsx')}")

    # 2. ç»˜åˆ¶æŸ±çŠ¶å›¾ï¼ˆåˆ†ä¸¤ç»„æŒ‡æ ‡ï¼‰
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

    # å­å›¾1ï¼šèšç±»æœ‰æ•ˆæ€§æŒ‡æ ‡ï¼ˆæ’é™¤è¯´æ˜åˆ—ï¼‰
    validity_plot = validity_df.drop('æŒ‡æ ‡è¯´æ˜', axis=1).set_index('æŒ‡æ ‡åç§°')
    x = np.arange(len(validity_plot.index))
    width = 0.35
    ax1.bar(x - width / 2, validity_plot['åŸå§‹KMeans'], width, label='åŸå§‹KMeans', color='#FF6B6B', alpha=0.8)
    ax1.bar(x + width / 2, validity_plot['æ”¹è¿›KMeans'], width, label='æ”¹è¿›KMeans', color='#4ECDC4', alpha=0.8)
    ax1.set_xlabel('èšç±»æœ‰æ•ˆæ€§æŒ‡æ ‡', fontsize=14, fontweight='bold')
    ax1.set_ylabel('æŒ‡æ ‡å€¼', fontsize=14, fontweight='bold')
    ax1.set_title('èšç±»æœ‰æ•ˆæ€§æŒ‡æ ‡å¯¹æ¯”', fontsize=16, fontweight='bold')
    ax1.set_xticks(x)
    ax1.set_xticklabels(validity_plot.index, rotation=15, ha='right')
    ax1.legend()
    ax1.grid(axis='y', alpha=0.3)

    # å­å›¾2ï¼šä¸šåŠ¡é€‚é…æŒ‡æ ‡ï¼ˆæ’é™¤è¯´æ˜åˆ—ï¼‰
    business_plot = business_df.drop('æŒ‡æ ‡è¯´æ˜', axis=1).set_index('æŒ‡æ ‡åç§°')
    x = np.arange(len(business_plot.index))
    ax2.bar(x - width / 2, business_plot['åŸå§‹KMeans'], width, label='åŸå§‹KMeans', color='#FF6B6B', alpha=0.8)
    ax2.bar(x + width / 2, business_plot['æ”¹è¿›KMeans'], width, label='æ”¹è¿›KMeans', color='#4ECDC4', alpha=0.8)
    ax2.set_xlabel('ä¸šåŠ¡é€‚é…æŒ‡æ ‡', fontsize=14, fontweight='bold')
    ax2.set_ylabel('æŒ‡æ ‡å€¼', fontsize=14, fontweight='bold')
    ax2.set_title('å¤§åæµ‹ç‚¹ä¸šåŠ¡é€‚é…æŒ‡æ ‡å¯¹æ¯”', fontsize=16, fontweight='bold')
    ax2.set_xticks(x)
    ax2.set_xticklabels(business_plot.index, rotation=15, ha='right')
    ax2.legend()
    ax2.grid(axis='y', alpha=0.3)

    plt.tight_layout()
    img_path = os.path.join(OUTPUT_DIR, "èšç±»æ–¹æ³•æŒ‡æ ‡å¯¹æ¯”å›¾.png")
    plt.savefig(img_path, dpi=600, bbox_inches='tight')
    plt.close()
    print(f"ğŸ“ˆ æŒ‡æ ‡å¯¹æ¯”å›¾å·²ä¿å­˜è‡³ï¼š{img_path}")

    # 3. è¾“å‡ºæ–‡å­—æ€»ç»“
    print("\n" + "=" * 60)
    print("èšç±»æ–¹æ³•å¯¹æ¯”æ€»ç»“")
    print("=" * 60)
    # æœ‰æ•ˆæ€§æŒ‡æ ‡æ€»ç»“
    print("\n1. èšç±»æœ‰æ•ˆæ€§æŒ‡æ ‡ï¼ˆé€šç”¨ï¼‰ï¼š")
    for _, row in validity_df.iterrows():
        print(
            f"   - {row['æŒ‡æ ‡åç§°']}ï¼šåŸå§‹KMeans={row['åŸå§‹KMeans']}ï¼Œæ”¹è¿›KMeans={row['æ”¹è¿›KMeans']}ï¼ˆ{row['æŒ‡æ ‡è¯´æ˜']}ï¼‰")
    # ä¸šåŠ¡æŒ‡æ ‡æ€»ç»“
    print("\n2. ä¸šåŠ¡é€‚é…æŒ‡æ ‡ï¼ˆå¤§ååœºæ™¯ï¼‰ï¼š")
    for _, row in business_df.iterrows():
        print(
            f"   - {row['æŒ‡æ ‡åç§°']}ï¼šåŸå§‹KMeans={row['åŸå§‹KMeans']}ï¼Œæ”¹è¿›KMeans={row['æ”¹è¿›KMeans']}ï¼ˆ{row['æŒ‡æ ‡è¯´æ˜']}ï¼‰")
    # ç»“è®ºï¼ˆåŸºäºæŒ‡æ ‡è¶‹åŠ¿ï¼‰
    improved_better = 0
    # æœ‰æ•ˆæ€§æŒ‡æ ‡åˆ¤æ–­
    if validity_df.iloc[0]['æ”¹è¿›KMeans'] > validity_df.iloc[0]['åŸå§‹KMeans']: improved_better += 1  # è½®å»“ç³»æ•°
    if validity_df.iloc[1]['æ”¹è¿›KMeans'] > validity_df.iloc[1]['åŸå§‹KMeans']: improved_better += 1  # Calinski
    if validity_df.iloc[2]['æ”¹è¿›KMeans'] < validity_df.iloc[2]['åŸå§‹KMeans']: improved_better += 1  # Davies-Bouldin
    # é™æ€è½®å»“ç³»æ•°ï¼ˆè‹¥å­˜åœ¨ï¼‰
    try:
        row_static = validity_df[validity_df['æŒ‡æ ‡åç§°'] == 'é™æ€è½®å»“ç³»æ•°ï¼ˆStatic-Silhouetteï¼‰']
        if len(row_static) == 1 and row_static.iloc[0]['æ”¹è¿›KMeans'] > row_static.iloc[0]['åŸå§‹KMeans']:
            improved_better += 1
    except Exception:
        pass
    # ä¸šåŠ¡æŒ‡æ ‡åˆ¤æ–­
    if business_df.iloc[0]['æ”¹è¿›KMeans'] > business_df.iloc[0]['åŸå§‹KMeans']: improved_better += 1  # æ—¶åºç›¸å…³
    if business_df.iloc[1]['æ”¹è¿›KMeans'] < business_df.iloc[1]['åŸå§‹KMeans']: improved_better += 1  # é™æ€è·ç¦»
    # è¾“å‡ºç»“è®º
    if improved_better >= 3:
        print("\nâœ… ç»“è®ºï¼šæ”¹è¿›KMeansåœ¨å¤šæ•°æŒ‡æ ‡ä¸Šä¼˜äºåŸå§‹KMeansï¼Œæ›´é€‚é…å¤§åæµ‹ç‚¹èšç±»éœ€æ±‚")
    else:
        print("\nâš ï¸  ç»“è®ºï¼šæ”¹è¿›KMeansæœªå®Œå…¨ä¼˜äºåŸå§‹KMeansï¼Œå»ºè®®æ£€æŸ¥æ•°æ®æˆ–è°ƒæ•´èšç±»å‚æ•°ï¼ˆå¦‚Kå€¼ï¼‰")
    print("=" * 60)


def generate_comparison_named(validity_df, business_df, name_a='æ–¹æ³•A', name_b='æ–¹æ³•B'):
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
    v_plot = validity_df.drop('æŒ‡æ ‡è¯´æ˜', axis=1).set_index('æŒ‡æ ‡åç§°')
    x = np.arange(len(v_plot.index))
    w = 0.35
    ax1.bar(x - w / 2, v_plot[name_a], w, label=name_a, color='#FF6B6B', alpha=0.8)
    ax1.bar(x + w / 2, v_plot[name_b], w, label=name_b, color='#4ECDC4', alpha=0.8)
    ax1.set_xlabel('èšç±»æœ‰æ•ˆæ€§æŒ‡æ ‡', fontsize=14, fontweight='bold')
    ax1.set_ylabel('æŒ‡æ ‡å€¼', fontsize=14, fontweight='bold')
    ax1.set_title('èšç±»æœ‰æ•ˆæ€§æŒ‡æ ‡å¯¹æ¯”', fontsize=16, fontweight='bold')
    ax1.set_xticks(x)
    ax1.set_xticklabels(v_plot.index, rotation=15, ha='right')
    ax1.legend()
    ax1.grid(axis='y', alpha=0.3)
    b_plot = business_df.drop('æŒ‡æ ‡è¯´æ˜', axis=1).set_index('æŒ‡æ ‡åç§°')
    x2 = np.arange(len(b_plot.index))
    ax2.bar(x2 - w / 2, b_plot[name_a], w, label=name_a, color='#FF6B6B', alpha=0.8)
    ax2.bar(x2 + w / 2, b_plot[name_b], w, label=name_b, color='#4ECDC4', alpha=0.8)
    ax2.set_xlabel('ä¸šåŠ¡é€‚é…æŒ‡æ ‡', fontsize=14, fontweight='bold')
    ax2.set_ylabel('æŒ‡æ ‡å€¼', fontsize=14, fontweight='bold')
    ax2.set_title('å¤§åæµ‹ç‚¹ä¸šåŠ¡é€‚é…æŒ‡æ ‡å¯¹æ¯”', fontsize=16, fontweight='bold')
    ax2.set_xticks(x2)
    ax2.set_xticklabels(b_plot.index, rotation=15, ha='right')
    ax2.legend()
    ax2.grid(axis='y', alpha=0.3)
    plt.tight_layout()
    tag_a = name_a.replace('/', '_')
    tag_b = name_b.replace('/', '_')
    excel_path = os.path.join(OUTPUT_DIR, f"èšç±»æ–¹æ³•å¯¹æ¯”æŒ‡æ ‡è¡¨_{tag_a}_vs_{tag_b}.xlsx")
    img_path = os.path.join(OUTPUT_DIR, f"èšç±»æ–¹æ³•æŒ‡æ ‡å¯¹æ¯”å›¾_{tag_a}_vs_{tag_b}.png")
    with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:
        validity_df.to_excel(writer, sheet_name='èšç±»æœ‰æ•ˆæ€§æŒ‡æ ‡', index=False)
        business_df.to_excel(writer, sheet_name='ä¸šåŠ¡é€‚é…æŒ‡æ ‡', index=False)
    plt.savefig(img_path, dpi=600, bbox_inches='tight')
    plt.close()
    return excel_path, img_path


def compare_methods(name_a, result_a_path, coupling_a_path, name_b, result_b_path, coupling_b_path,
                    excel_path="./monitor_data.xlsx"):
    result_a, coupling_a, result_b, coupling_b = load_results_generic(result_a_path, coupling_a_path, result_b_path,
                                                                      coupling_b_path)
    sheet_data = load_time_series_data(excel_path)
    v_df = compute_validity_metrics_named(result_a, result_b, sheet_data, name_a, name_b)
    b_df = compute_business_metrics_named(coupling_a, coupling_b, name_a, name_b)
    return generate_comparison_named(v_df, b_df, name_a, name_b)


# ä¸»å‡½æ•°ï¼ˆç‹¬ç«‹è¿è¡Œå…¥å£ï¼‰
def main():
    try:
        print("=" * 50)
        print("å¼€å§‹æ‰§è¡Œä¸¤ç§èšç±»æ–¹æ³•çš„æŒ‡æ ‡å¯¹æ¯”")
        print("=" * 50)
        # 1. è¯»å–ç»“æœ
        original_result, original_coupling, improved_result, improved_coupling = load_results()
        # 2. è®¡ç®—æŒ‡æ ‡
        # è¯»å–æ—¶åºæ•°æ®ï¼ˆç”¨äºé™æ€è½®å»“ç³»æ•°ï¼‰
        sheet_data = load_time_series_data()
        validity_df = compute_validity_metrics(original_result, improved_result, sheet_data)
        business_df = compute_business_metrics(original_coupling, improved_coupling)
        # 3. ç”Ÿæˆå¯¹æ¯”ç»“æœ
        generate_comparison(validity_df, business_df)
        print("\n" + "=" * 50)
        print("æŒ‡æ ‡å¯¹æ¯”æ‰§è¡Œå®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³ï¼šcluster_comparison_results")
        print("=" * 50)
    except Exception as e:
        print(f"[å¯¹æ¯”è„šæœ¬] æ‰§è¡Œå‡ºé”™ï¼š{str(e)}")


def run_param_grid(
        excel_path="./monitor_data.xlsx",
        n_clusters=3,
        alpha_list=(0.4, 0.5, 0.6),
        beta_list=(0.3, 0.35, 0.4),
        gamma_list=(0.3, 0.35, 0.4),
        knn_list=(None, 3, 5),
        min_sep_ratio_list=(0.5, 1 / 3),
        max_iter=50,
        tol=1e-5,
        output_csv=os.path.join(OUTPUT_DIR, "å‚æ•°ç½‘æ ¼å¯¹æ¯”ç»“æœ.csv"),
        required_better_count=3
):
    try:
        sheet_data = improved_kmeans.load_data(excel_path)
        std_data = improved_kmeans.normalize_temporal(sheet_data)
        features_df = improved_kmeans.extract_features(sheet_data)

        original_features = origin_kmeans.extract_features(sheet_data)
        original_result, _ = origin_kmeans.original_kmeans(original_features, n_clusters)
        original_coupling = origin_kmeans.compute_coupling(original_result, sheet_data)

        random_features = random_kmeans.extract_features(sheet_data)
        random_result, _ = random_kmeans.random_kmeans(random_features, n_clusters)
        random_coupling = random_kmeans.compute_coupling(random_result, sheet_data)

        os.makedirs(OUTPUT_DIR, exist_ok=True)
        columns = [
            'alpha', 'beta', 'gamma', 'knn_k', 'min_sep_ratio', 'n_clusters',
            'silhouette_original', 'silhouette_improved',
            'calinski_original', 'calinski_improved',
            'davies_original', 'davies_improved',
            'static_silhouette_original', 'static_silhouette_improved',
            'avg_corr_original', 'avg_corr_improved',
            'avg_static_dist_original', 'avg_static_dist_improved',
            'better_or_equal_count', 'considered_metric_count'
        ]
        need_header = not os.path.exists(output_csv) or os.path.getsize(output_csv) == 0
        f = open(output_csv, 'a', newline='', encoding='utf-8-sig')
        writer = csv.DictWriter(f, fieldnames=columns)
        if need_header:
            writer.writeheader()
        for alpha in alpha_list:
            for beta in beta_list:
                for gamma in gamma_list:
                    if beta < 0 or gamma < 0:
                        continue
                    for knn_k in knn_list:
                        for min_sep_ratio in min_sep_ratio_list:
                            improved_result, _, _, _ = improved_kmeans.improved_kmeans(
                                features_df, std_data, n_clusters,
                                max_iter=max_iter, tol=tol,
                                alpha=alpha, beta=beta, gamma=gamma, knn_k=knn_k, min_sep_ratio=min_sep_ratio
                            )
                            improved_coupling = improved_kmeans.compute_coupling(improved_result, std_data)
                            # validity_df = compute_validity_metrics(original_result, improved_result,
                            #                                        load_time_series_data(excel_path))
                            validity_df = compute_validity_metrics(random_result, improved_result,
                                                                   load_time_series_data(excel_path))
                            # business_df = compute_business_metrics(original_coupling, improved_coupling)
                            business_df = compute_business_metrics(random_coupling, improved_coupling)

                            def v(name):
                                row = validity_df[validity_df['æŒ‡æ ‡åç§°'] == name]
                                return (row.iloc[0]['åŸå§‹KMeans'], row.iloc[0]['æ”¹è¿›KMeans']) if len(row) == 1 else (
                                    np.nan, np.nan)

                            sil_o, sil_i = v('è½®å»“ç³»æ•°ï¼ˆSilhouetteï¼‰')
                            ch_o, ch_i = v('Calinski-Harabasz')
                            db_o, db_i = v('Davies-Bouldin')
                            ss_o, ss_i = v('é™æ€è½®å»“ç³»æ•°ï¼ˆStatic-Silhouetteï¼‰')
                            b_row_corr = business_df[business_df['æŒ‡æ ‡åç§°'] == 'ç°‡å†…å¹³å‡æ—¶åºç›¸å…³ç³»æ•°']
                            b_row_dist = business_df[business_df['æŒ‡æ ‡åç§°'] == 'ç°‡å†…å¹³å‡é™æ€è¿‘é‚»è·ç¦»']
                            corr_o = b_row_corr.iloc[0]['åŸå§‹KMeans'] if len(b_row_corr) == 1 else np.nan
                            corr_i = b_row_corr.iloc[0]['æ”¹è¿›KMeans'] if len(b_row_corr) == 1 else np.nan
                            dist_o = b_row_dist.iloc[0]['åŸå§‹KMeans'] if len(b_row_dist) == 1 else np.nan
                            dist_i = b_row_dist.iloc[0]['æ”¹è¿›KMeans'] if len(b_row_dist) == 1 else np.nan
                            better_count = 0
                            considered = 0
                            if not np.isnan(sil_o) and not np.isnan(sil_i):
                                considered += 1
                                if sil_i >= sil_o: better_count += 1
                            if not np.isnan(ch_o) and not np.isnan(ch_i):
                                considered += 1
                                if ch_i >= ch_o: better_count += 1
                            if not np.isnan(db_o) and not np.isnan(db_i):
                                considered += 1
                                if db_i <= db_o: better_count += 1
                            if not np.isnan(ss_o) and not np.isnan(ss_i):
                                considered += 1
                                if ss_i >= ss_o: better_count += 1
                            if not np.isnan(corr_o) and not np.isnan(corr_i):
                                considered += 1
                                if corr_i >= corr_o: better_count += 1
                            if not np.isnan(dist_o) and not np.isnan(dist_i):
                                considered += 1
                                if dist_i <= dist_o: better_count += 1
                            if better_count >= required_better_count:
                                writer.writerow({
                                    'alpha': alpha,
                                    'beta': beta,
                                    'gamma': gamma,
                                    'knn_k': -1 if knn_k is None else knn_k,
                                    'min_sep_ratio': float(min_sep_ratio),
                                    'n_clusters': n_clusters,
                                    'silhouette_original': sil_o,
                                    'silhouette_improved': sil_i,
                                    'calinski_original': ch_o,
                                    'calinski_improved': ch_i,
                                    'davies_original': db_o,
                                    'davies_improved': db_i,
                                    'static_silhouette_original': ss_o,
                                    'static_silhouette_improved': ss_i,
                                    'avg_corr_original': corr_o,
                                    'avg_corr_improved': corr_i,
                                    'avg_static_dist_original': dist_o,
                                    'avg_static_dist_improved': dist_i,
                                    'better_or_equal_count': better_count,
                                    'considered_metric_count': considered
                                })
        f.close()
        print(f"å‚æ•°ç½‘æ ¼å¯¹æ¯”ç»“æœå·²è¿½åŠ å†™å…¥ï¼š{output_csv}")
        return output_csv
    except Exception as e:
        print(f"[å‚æ•°ç½‘æ ¼] æ‰§è¡Œå‡ºé”™ï¼š{str(e)}")
        return None


if __name__ == "__main__":
    run_param_grid(excel_path="./monitor_data.xlsx",
                   n_clusters=3,
                   alpha_list=[0.3, 0.4, 0.5],
                   beta_list=[0.3, 0.4, 0.5],
                   gamma_list=[0.3, 0.4, 0.5], knn_list=[5, 7, 9],
                   min_sep_ratio_list=[0.1, 0.25],
                   required_better_count=4)
